{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWFn8iwW3oVr",
        "outputId": "1a2044b7-9b08-43b0-955f-72c63f95a630"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.42.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.4.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentence-transformers\n",
            "Successfully installed sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import re\n",
        "\n",
        "def fetch_headlines(api_key, keyword):\n",
        "    url = f\"https://newsapi.org/v2/everything?q={keyword}&apiKey={api_key}\"\n",
        "    response = requests.get(url)\n",
        "    articles = response.json().get('articles', [])\n",
        "    headlines = [article['title'] for article in articles]\n",
        "    return headlines\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'\\W+', ' ', text)  # Remove special characters\n",
        "    text = text.lower().strip()  # Convert to lowercase and strip extra spaces\n",
        "    return text\n",
        "\n",
        "def load_opt_model():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def paraphrase_text(text, model, tokenizer):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    outputs = model.generate(**inputs, max_length=512, num_beams=5, early_stopping=True)\n",
        "    paraphrased_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"Paraphrased Text: {paraphrased_text}\")  # Debugging line\n",
        "    return paraphrased_text\n",
        "\n",
        "def load_sentence_model():\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    return model\n",
        "\n",
        "def get_similarity(text1, text2, model):\n",
        "    embeddings = model.encode([text1, text2], convert_to_tensor=True)\n",
        "    return util.pytorch_cos_sim(embeddings[0], embeddings[1]).item()\n",
        "\n",
        "def is_matching(user_input, headlines, model, tokenizer, sentence_model):\n",
        "    preprocessed_input = preprocess_text(user_input)\n",
        "    paraphrased_input = paraphrase_text(preprocessed_input, model, tokenizer)\n",
        "\n",
        "    for headline in headlines:\n",
        "        preprocessed_headline = preprocess_text(headline)\n",
        "        similarity = get_similarity(paraphrased_input, preprocessed_headline, sentence_model)\n",
        "        if similarity > 0.8:  # Threshold for similarity\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Example usage\n",
        "api_key = 'Give Your Own Key'\n",
        "keyword = 'Indian Navy Instagram nuke reveal'\n",
        "user_input = 'Indian Navy accidentally reveals nuke information in an Instagram post.'\n",
        "\n",
        "model, tokenizer = load_opt_model()\n",
        "sentence_model = load_sentence_model()\n",
        "headlines = fetch_headlines(api_key, keyword)\n",
        "match = is_matching(user_input, headlines, model, tokenizer, sentence_model)\n",
        "\n",
        "if match:\n",
        "    print(\"The user input matches with a news headline.\")\n",
        "else:\n",
        "    print(\"The user input does not match any news headline.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8_Pc_LxpfkX",
        "outputId": "5b8b87d9-0ec8-4f3d-80ed-6f0124ba501b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paraphrased Text: indian navy accidentally reveals nuke information in an instagram post\n",
            "\n",
            "indian navy accidentally reveals nuke information in an instagram post\n",
            "\n",
            "indian navy accidentally reveals nuke information in an instagram post\n",
            "\n",
            "indian navy accidentally reveals nuke information in an instagram post\n",
            "\n",
            "indian navy accidentally reveals nuke information in an instagram post\n",
            "The user input matches with a news headline.\n"
          ]
        }
      ]
    }
  ]
}